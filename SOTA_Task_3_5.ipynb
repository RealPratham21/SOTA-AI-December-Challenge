{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9dzODZLlFCR"
   },
   "source": [
    "# Reconstructing Exam Questions from Model-Generated Answers  \n",
    "### SOTA-AI December Task 3 — David’s Archive Issue\n",
    "\n",
    "This notebook presents a complete, phase-wise solution to the inverse question–answering task posed in the SOTA-AI December Challenge.\n",
    "\n",
    "The objective is to reconstruct plausible, exam-style questions given answers generated by a large language model. Since the original questions are unavailable and evaluation is based on semantic similarity rather than exact string matching, the task requires reasoning about intent, meaning, and the inversion of the question–answer relationship rather than surface-level text matching.\n",
    "\n",
    "The solution is organized into clearly defined stages:\n",
    "1. Understanding the structure and characteristics of the dataset  \n",
    "2. Constructing a synthetic supervision signal from unlabeled answers  \n",
    "3. Fine-tuning a language model using parameter-efficient methods  \n",
    "4. Generating questions for the test set  \n",
    "5. Performing selective semantic repair using cycle consistency  \n",
    "6. Auditing outputs and preparing the final submission  \n",
    "\n",
    "All stages are designed to be conceptually complete and reproducible. For practical execution under constrained compute environments, the same logic can be applied in batches by operating on subsets of the data. Since all processing steps are independent per example, batch-wise execution yields identical results to a single end-to-end run, and final outputs are obtained by aggregating and ordering these batches deterministically.\n",
    "\n",
    "The emphasis throughout this notebook is on methodological clarity, semantic correctness, and principled decision-making under realistic resource constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d2XZmYSc54v"
   },
   "source": [
    "## 1. Understanding the Dataset\n",
    "\n",
    "The task is an inverse question-answering problem: given an answer generated by a large language model, the goal is to reconstruct a plausible exam-style question that could have produced that answer.\n",
    "\n",
    "The dataset consists of:\n",
    "- A **training set** containing only model-generated answers (no questions).\n",
    "- A **test set** containing unseen answers for which questions must be generated.\n",
    "\n",
    "Key observations from exploratory analysis:\n",
    "- Answers are long, explanatory, and often multi-paragraph.\n",
    "- Many answers implicitly encode the question they are responding to.\n",
    "- There is no direct supervision signal (question → answer pairs), making this a fundamentally ill-posed inverse problem.\n",
    "\n",
    "Because evaluation is based on **semantic similarity** rather than exact string matching, the challenge is not surface-level phrasing, but capturing the *intent* of the original question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip /content/sota-ai-december-task-3-davids-archive-issue.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the train and test CSV files\n",
    "\n",
    "train_path = \"/content/kaggle_dataset/train.csv\"\n",
    "test_path  = \"/content/kaggle_dataset/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape :\", test_df.shape)\n",
    "\n",
    "train_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values in the datasets\n",
    "\n",
    "print(train_df.info())\n",
    "print(\"\\nNulls in train:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nNulls in test:\")\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the word count distribution in the answers\n",
    "\n",
    "def word_count(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "train_df[\"word_count\"] = train_df[\"ans\"].apply(word_count)\n",
    "test_df[\"word_count\"]  = test_df[\"ans\"].apply(word_count)\n",
    "\n",
    "train_df[\"word_count\"].describe(), test_df[\"word_count\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the word count distribution\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(train_df[\"word_count\"], bins=50, alpha=0.7, label=\"train\")\n",
    "plt.hist(test_df[\"word_count\"], bins=50, alpha=0.7, label=\"test\")\n",
    "plt.legend()\n",
    "plt.title(\"Answer Length Distribution (words)\")\n",
    "plt.xlabel(\"Word count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features: presence of lists and whether the answer starts with a definition phrase\n",
    "\n",
    "def has_list(text):\n",
    "    return bool(re.search(r\"\\n\\d+\\.|\\n- |\\n\\*\", text))\n",
    "\n",
    "def starts_definition(text):\n",
    "    return text.lower().strip().startswith(\n",
    "        (\"in the\", \"the term\", \"the concept\", \"abduction\", \"predictive\")\n",
    "    )\n",
    "\n",
    "train_df[\"has_list\"] = train_df[\"ans\"].apply(has_list)\n",
    "train_df[\"starts_definition\"] = train_df[\"ans\"].apply(starts_definition)\n",
    "\n",
    "train_df[[\"has_list\", \"starts_definition\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing sentence and paragraph counts in the answers\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(re.findall(r\"[.!?]\", text))\n",
    "\n",
    "def paragraph_count(text):\n",
    "    return len([p for p in text.split(\"\\n\") if p.strip()])\n",
    "\n",
    "train_df[\"sentences\"] = train_df[\"ans\"].apply(sentence_count)\n",
    "train_df[\"paragraphs\"] = train_df[\"ans\"].apply(paragraph_count)\n",
    "\n",
    "train_df[[\"sentences\", \"paragraphs\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword presence analysis in the answers and their frequencies\n",
    "\n",
    "keywords = [\n",
    "    \"define\", \"definition\", \"example\", \"compare\", \"difference\",\n",
    "    \"advantage\", \"disadvantage\", \"bias\", \"limitation\",\n",
    "    \"ethics\", \"alignment\", \"reasoning\", \"inductive\", \"abductive\"\n",
    "]\n",
    "\n",
    "def keyword_hits(text):\n",
    "    text = text.lower()\n",
    "    return {k: (k in text) for k in keywords}\n",
    "\n",
    "kw_df = train_df[\"ans\"].apply(keyword_hits).apply(pd.Series)\n",
    "kw_df.mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying sample answers from the training set\n",
    "\n",
    "for i in np.random.choice(len(train_df), 5, replace=False):\n",
    "    print(\"=\"*80)\n",
    "    print(train_df.loc[i, \"quesid\"])\n",
    "    print(train_df.loc[i, \"ans\"][:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the answers: normalizing line breaks and removing extra spaces\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\r\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "train_df[\"ans_clean\"] = train_df[\"ans\"].apply(clean_answer)\n",
    "test_df[\"ans_clean\"]  = test_df[\"ans\"].apply(clean_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing key statistics about the datasets\n",
    "\n",
    "def word_count(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "train_df[\"word_count\"] = train_df[\"ans\"].apply(word_count)\n",
    "test_df[\"word_count\"]  = test_df[\"ans\"].apply(word_count)\n",
    "\n",
    "PHASE_1_SUMMARY = {\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"median_word_count_train\": train_df[\"word_count\"].median(),\n",
    "    \"median_word_count_test\": test_df[\"word_count\"].median(),\n",
    "    \"multi_paragraph_fraction\": float((train_df[\"paragraphs\"] > 1).mean()),\n",
    "    \"list_fraction\": float(train_df[\"has_list\"].mean()),\n",
    "}\n",
    "\n",
    "PHASE_1_SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3UHOkOQSK8X"
   },
   "source": [
    "## 2. Generating Synthetic Question–Answer Pairs\n",
    "\n",
    "Since the training data does not include ground-truth questions, a synthetic supervision signal is required.\n",
    "\n",
    "To construct this signal:\n",
    "- Multiple candidate questions are generated for each training answer using a strong prompt.\n",
    "- A cycle-consistency heuristic is applied:\n",
    "  - Generated question → regenerated answer\n",
    "  - Semantic similarity is measured between the regenerated answer and the original answer.\n",
    "- Only high-quality question–answer pairs are retained.\n",
    "\n",
    "This process prioritizes **quality over quantity**, producing a small but reliable dataset of synthetic QA pairs that reflect the structure and intent of the original answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "\n",
    "!pip install -q transformers accelerate bitsandbytes peft sentence-transformers tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Weights & Biases for experiment tracking\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sota-ai-task3-phase2\",\n",
    "    name=\"phase2-debug-300-samples\",\n",
    "    config={\n",
    "        \"model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"num_samples\": 300,\n",
    "        \"num_candidates\": 2,\n",
    "        \"temperatures\": [0.0, 0.6],\n",
    "        \"max_q_tokens\": 64,\n",
    "        \"max_a_tokens\": 256\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip /content/sota-ai-december-task-3-davids-archive-issue.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the train dataset\n",
    "\n",
    "train_df = pd.read_csv(\"/content/kaggle_dataset/train.csv\")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the answers: normalizing line breaks and removing extra spaces\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\r\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "train_df[\"ans_clean\"] = train_df[\"ans\"].apply(clean_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling the training data to ensure complete run in Colab's constrained resources\n",
    "\n",
    "N_SAMPLES = 275\n",
    "train_subset = train_df.sample(n=N_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Subset size:\", len(train_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Llama 3.1 8B Instruct model and tokenizer. Using LLaMA 3.1 8B Instruct because it is a state-of-the-art model for instruction following and is also the model mentioned in the problem. Using 4-bit quantization for efficiency.\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sentence transformer for embedding generation\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "BASE_PROMPT = \"\"\"You are an academic exam setter.\n",
    "\n",
    "Given the following answer written in a formal, philosophical or analytical style, write a single well-formed exam question that this answer would plausibly respond to.\n",
    "\n",
    "Guidelines:\n",
    "- Use an interrogative form (What / How / Why / Explain / Discuss).\n",
    "- Keep the question to ONE sentence.\n",
    "- Do not introduce facts not present in the answer.\n",
    "- End with a question mark.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining helper functions. extract_question extracts the first plausible question from the model output, handling noisy formatting. generate_2_questions generates two candidate questions using different temperatures. regenerate_answer generates an answer for a given question. semantic_similarity computes the cosine similarity between two texts using sentence embeddings.\n",
    "\n",
    "def extract_question(text):\n",
    "    \"\"\"\n",
    "    Extract the FIRST plausible question from model output.\n",
    "    Handles noisy LLM formatting.\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        # strip common prefixes\n",
    "        line = re.sub(r\"^(Question:|Q:|Here is.*?:|Possible question:)\", \"\", line).strip()\n",
    "\n",
    "        if \"?\" in line:\n",
    "            q = line.split(\"?\")[0].strip() + \"?\"\n",
    "            if len(q.split()) >= 4:\n",
    "                return q\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_2_questions(answer, log=False):\n",
    "    temperatures = [0.0, 0.6]\n",
    "    questions = []\n",
    "\n",
    "    for temp in temperatures:\n",
    "        prompt = BASE_PROMPT.format(answer=answer)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=(temp > 0),\n",
    "            temperature=temp if temp > 0 else None,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        q = extract_question(decoded)\n",
    "\n",
    "        if log:\n",
    "            print(\"\\n--- RAW MODEL OUTPUT ---\")\n",
    "            print(decoded[:500])\n",
    "            print(\"--- EXTRACTED QUESTION ---\")\n",
    "            print(q)\n",
    "\n",
    "        if q:\n",
    "            questions.append(q)\n",
    "\n",
    "    # dedupe while preserving order\n",
    "    return list(dict.fromkeys(questions))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def regenerate_answer(question):\n",
    "    prompt = f\"\"\"You are answering an academic exam question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "\n",
    "def semantic_similarity(a, b):\n",
    "    ea = embedder.encode(a, convert_to_tensor=True)\n",
    "    eb = embedder.encode(b, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(ea, eb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: generating questions, regenerating answers, computing similarity, and logging results.\n",
    "\n",
    "synthetic_rows = []\n",
    "\n",
    "DEBUG_FIRST_N = 275   # log everything\n",
    "\n",
    "for idx, row in tqdm(train_subset.iterrows(), total=len(train_subset)):\n",
    "    answer = row[\"ans_clean\"]\n",
    "\n",
    "    log = idx < DEBUG_FIRST_N\n",
    "    questions = generate_2_questions(answer, log=log)\n",
    "\n",
    "    # log number of questions generated\n",
    "    wandb.log({\n",
    "        \"num_questions_generated\": len(questions)\n",
    "    })\n",
    "\n",
    "    if log:\n",
    "        print(\"\\nGenerated questions:\", questions)\n",
    "\n",
    "    for q in questions:\n",
    "        regen_ans = regenerate_answer(q)\n",
    "        sim = semantic_similarity(answer, regen_ans)\n",
    "\n",
    "        if log:\n",
    "            print(\"\\nQUESTION:\", q)\n",
    "            print(\"REGENERATED ANSWER (truncated):\", regen_ans[:400])\n",
    "            print(\"SEMANTIC SIMILARITY:\", sim)\n",
    "\n",
    "        synthetic_rows.append({\n",
    "            \"quesid\": row[\"quesid\"],\n",
    "            \"answer\": answer,\n",
    "            \"question\": q,\n",
    "            \"cycle_score\": sim\n",
    "        })\n",
    "\n",
    "        # per-question W&B logging\n",
    "        wandb.log({\n",
    "            \"cycle_similarity\": sim,\n",
    "            \"question_length\": len(q.split()),\n",
    "            \"answer_length\": len(answer.split())\n",
    "        })\n",
    "\n",
    "    # occasional text logging (every 25 samples)\n",
    "    if idx % 25 == 0 and questions:\n",
    "        wandb.log({\n",
    "            \"sample_answer\": wandb.Html(answer[:600]),\n",
    "            \"sample_question\": questions[0]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from the synthetic rows and displaying summary statistics\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_rows)\n",
    "\n",
    "print(\"Total QA pairs:\", len(synthetic_df))\n",
    "synthetic_df[\"cycle_score\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"total_QA_pairs\": len(synthetic_df),\n",
    "    \"mean_cycle_score\": synthetic_df[\"cycle_score\"].mean(),\n",
    "    \"median_cycle_score\": synthetic_df[\"cycle_score\"].median()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying random samples from the synthetic dataset\n",
    "\n",
    "for i in np.random.choice(len(synthetic_df), 5, replace=False):\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANSWER:\\n\", synthetic_df.iloc[i][\"answer\"][:600])\n",
    "    print(\"\\nQUESTION:\\n\", synthetic_df.iloc[i][\"question\"])\n",
    "    print(\"SIM:\", synthetic_df.iloc[i][\"cycle_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the synthetic dataset to a CSV file and finishing the W&B run. The synthetic dataset is used for training in the next phase.\n",
    "\n",
    "synthetic_df.to_csv(\"/content/synthetic_AQ_debug.csv\", index=False)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/synthetic_AQ_debug.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVaTdTPSVuK"
   },
   "source": [
    "## 3. Fine-Tuning the Language Model (QLoRA)\n",
    "\n",
    "To bias the model toward clean, exam-style inverse question generation, the synthetic QA pairs are used to fine-tune a LLaMA-based model.\n",
    "\n",
    "Key design choices:\n",
    "- **QLoRA** is used for efficiency, allowing fine-tuning on limited compute.\n",
    "- Only a small number of parameters are trained, reducing overfitting risk.\n",
    "- Training focuses on:\n",
    "  - Learning the answer → question inversion pattern\n",
    "  - Producing a single, well-formed question\n",
    "  - Stopping generation cleanly without explanations or options\n",
    "\n",
    "Despite the small dataset size, validation loss trends indicate stable learning and good generalization for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes peft datasets sentence-transformers wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Weights & Biases for experiment tracking\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sota-ai-task3\",\n",
    "    name=\"qlora-finetune-275\",\n",
    "    config={\n",
    "        \"base_model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"method\": \"QLoRA\",\n",
    "        \"epochs\": 3,\n",
    "        \"samples\": 275\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the synthetic dataset and displaying its head\n",
    "\n",
    "synthetic_df = pd.read_csv(\"/content/synthetic_AQ_debug.csv\")\n",
    "\n",
    "print(\"Raw rows:\", len(synthetic_df))\n",
    "synthetic_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best question-answer pair per original question based on cycle score\n",
    "\n",
    "best_df = (\n",
    "    synthetic_df\n",
    "    .sort_values(\"cycle_score\", ascending=False)\n",
    "    .groupby(\"quesid\")\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"After grouping:\", len(best_df))\n",
    "best_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the lengths of answers and questions in the best dataset\n",
    "\n",
    "best_df[\"answer_len\"] = best_df[\"answer\"].apply(lambda x: len(x.split()))\n",
    "best_df[\"question_len\"] = best_df[\"question\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "best_df[[\"answer_len\", \"question_len\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the question length distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(best_df[\"question_len\"], bins=20)\n",
    "plt.title(\"Question Length Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying random samples from the best dataset\n",
    "\n",
    "for i in np.random.choice(len(best_df), 5, replace=False):\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANSWER:\\n\", best_df.iloc[i][\"answer\"][:600])\n",
    "    print(\"\\nQUESTION:\\n\", best_df.iloc[i][\"question\"])\n",
    "    print(\"CYCLE SCORE:\", best_df.iloc[i][\"cycle_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the best dataset into training and validation sets\n",
    "\n",
    "train_df = best_df.sample(frac=0.9, random_state=42)\n",
    "val_df   = best_df.drop(train_df.index)\n",
    "\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Val  :\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the dataset for Model Training and applying it to training and validation sets\n",
    "\n",
    "def format_example(row):\n",
    "    return {\n",
    "        \"text\": f\"\"\"You are an academic exam setter.\n",
    "\n",
    "TASK:\n",
    "Given the answer below, write ONE exam question.\n",
    "\n",
    "STRICT RULES:\n",
    "- Output ONLY the question.\n",
    "- Do NOT include multiple-choice options.\n",
    "- Do NOT include an answer.\n",
    "- Do NOT include explanations.\n",
    "- End with a question mark and STOP.\n",
    "\n",
    "Answer:\n",
    "{row['answer']}\n",
    "\n",
    "Question:\n",
    "{row['question']}\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.apply(format_example, axis=1, result_type=\"expand\"))\n",
    "val_ds   = Dataset.from_pandas(val_df.apply(format_example, axis=1, result_type=\"expand\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Llama 3.1 8B Instruct model in 4-bit quantized and tokenizer for QLoRA fine-tuning\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up LoRA configuration and applying it to the model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # rank is 16 for a good balance between performance and efficiency\n",
    "    lora_alpha=32, # scaling factor is 32 since it's double the rank as per best practices\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # target attention projection layers\n",
    "    lora_dropout=0.05, # small dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the dataset and applying it to training and validation sets\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "val_ds   = val_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up training arguments for the Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-out\",\n",
    "    num_train_epochs=3, # 3 epochs for sufficient fine-tuning under Colab constraints\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, # effective batch size of 8 to avoid OOM\n",
    "    per_device_eval_batch_size=1, # 1 for evaluation to manage memory\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10, # Transparent logging every 10 steps\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4, # Balanced learning rate for stable convergence\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"qlora-275\",\n",
    "    load_best_model_at_end=True, # load best model after training\n",
    "    save_total_limit=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Trainer and starting the training process\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function generate_question_strict generates a single exam question from a given answer using strict formatting rules to ensure clarity and relevance. The prompt is similar to previous question generation prompts but emphasizes strict adherence to output guidelines and leverages fine tuned model capabilities.\n",
    "\n",
    "def generate_question_strict(answer, model):\n",
    "    prompt = f\"\"\"You are an academic exam setter.\n",
    "\n",
    "TASK:\n",
    "Given the answer below, write ONE exam question.\n",
    "\n",
    "STRICT RULES:\n",
    "- Output ONLY the question.\n",
    "- Do NOT include options.\n",
    "- Do NOT include answers.\n",
    "- Do NOT include explanations.\n",
    "- End with a question mark and STOP.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=40,           # HARD CAP\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    q = decoded.split(\"Question:\")[-1].strip()\n",
    "\n",
    "    # HARD POST-PROCESSING SAFETY\n",
    "    q = q.split(\"?\")[0].strip() + \"?\"\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying random samples of generated questions from the validation set\n",
    "\n",
    "for i in np.random.choice(len(val_df), 5, replace=False):\n",
    "    ans = val_df.iloc[i][\"answer\"]\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANSWER:\\n\", ans[:400])\n",
    "    print(\"\\nGENERATED QUESTION:\\n\", generate_question_strict(ans, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the fine-tuned model and tokenizer, and finishing the W&B run\n",
    "\n",
    "model.save_pretrained(\"/content/qlora-strict-adapters\")\n",
    "tokenizer.save_pretrained(\"/content/qlora-strict-adapters\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/qlora-adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rJCdM1_TPvW"
   },
   "source": [
    "## 4. Test-Time Question Generation\n",
    "\n",
    "The fine-tuned model is used to generate questions for the test set answers.\n",
    "\n",
    "For each answer:\n",
    "- A single exam-style question is generated.\n",
    "- The generation process is deterministic to ensure consistency.\n",
    "- The output is constrained to produce exactly one question ending with a question mark.\n",
    "\n",
    "For practical execution in limited compute environments, inference can be performed in batches. However, the logic shown here generalizes to the full test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes peft sentence-transformers tqdm wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip /content/SOTA-3-FT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and initializing Weights & Biases for experiment tracking. Also setting the start and end indices for batch inference (The indices can be adjusted as needed depending on many examples you want to process in a single run).\n",
    "\n",
    "import wandb\n",
    "\n",
    "START = 2825\n",
    "END   = 3325   # adjust per session\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sota-ai-task3\",\n",
    "    name=\"test-inference-batch-0-400\",\n",
    "    config={\n",
    "        \"base_model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"adapters\": \"qlora-strict\",\n",
    "        \"generation_mode\": \"single_question_strict\",\n",
    "        \"start_idx\": START,\n",
    "        \"end_idx\": END,\n",
    "        \"batch_size\": END - START\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test dataset and displaying its head\n",
    "\n",
    "test_df = pd.read_csv(\"/content/test.csv\")\n",
    "print(\"Test size:\", len(test_df))\n",
    "test_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a slice of the test dataset for batch inference\n",
    "\n",
    "test_slice = test_df.iloc[START:END].reset_index(drop=True)\n",
    "print(f\"Processing rows [{START}:{END}) → {len(test_slice)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fine-tuned model with adapters for inference. Also loading the tokenizer.\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "ADAPTER_PATH = \"/content/SOTA-3-FT\"  # upload & unzip here\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model + adapters loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sentence transformer for embedding generation. generate_question_strict converts the test dataset to the prompt format and generates a question using the fine-tuned model. The prompt is based on the fine tuned model capabilities.\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def generate_question_strict(answer):\n",
    "    prompt = f\"\"\"You are an academic exam setter.\n",
    "\n",
    "TASK:\n",
    "Given the answer below, write ONE exam question.\n",
    "\n",
    "STRICT RULES:\n",
    "- Output ONLY the question.\n",
    "- Do NOT include options.\n",
    "- Do NOT include answers.\n",
    "- Do NOT include explanations.\n",
    "- End with a question mark and STOP.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=40,      # HARD CAP\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing safety net\n",
    "    q = decoded.split(\"Question:\")[-1].strip()\n",
    "    q = q.split(\"?\")[0].strip() + \"?\"\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the cycle_score function to compute semantic similarity between original and regenerated answers\n",
    "\n",
    "def cycle_score(original_answer, generated_question):\n",
    "    prompt = f\"\"\"Answer the following academic exam question.\n",
    "\n",
    "Question:\n",
    "{generated_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    regen = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    regen = regen.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    emb1 = embedder.encode(original_answer, convert_to_tensor=True)\n",
    "    emb2 = embedder.encode(regen, convert_to_tensor=True)\n",
    "\n",
    "    return float(util.cos_sim(emb1, emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main inference loop: generating questions, computing cycle scores, and logging results.\n",
    "\n",
    "results = []\n",
    "\n",
    "cycle_scores = []\n",
    "question_lengths = []\n",
    "\n",
    "for i, row in tqdm(test_slice.iterrows(), total=len(test_slice)):\n",
    "    ans = row[\"ans\"]\n",
    "    qid = row[\"quesid\"]\n",
    "\n",
    "    question = generate_question_strict(ans)\n",
    "\n",
    "    try:\n",
    "        cs = cycle_score(ans, question) \n",
    "    except Exception:\n",
    "        cs = None\n",
    "\n",
    "    results.append({\n",
    "        \"quesid\": qid,\n",
    "        \"question\": question,\n",
    "        \"cycle_score\": cs\n",
    "    })\n",
    "\n",
    "    # collect stats\n",
    "    if cs is not None:\n",
    "        cycle_scores.append(cs)\n",
    "    question_lengths.append(len(question.split()))\n",
    "\n",
    "    # log per-sample (lightweight)\n",
    "    wandb.log({\n",
    "        \"question_length\": len(question.split()),\n",
    "        \"cycle_score\": cs if cs is not None else -1\n",
    "    })\n",
    "\n",
    "    # rich logging every 25 samples\n",
    "    if i % 25 == 0:\n",
    "        wandb.log({\n",
    "            \"sample_quesid\": qid,\n",
    "            \"sample_question\": question,\n",
    "            \"sample_cycle_score\": cs\n",
    "        })\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(\"QID:\", qid)\n",
    "        print(\"QUESTION:\", question)\n",
    "        print(\"CYCLE SCORE:\", cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"avg_cycle_score\": np.mean(cycle_scores) if cycle_scores else None,\n",
    "    \"median_cycle_score\": np.median(cycle_scores) if cycle_scores else None,\n",
    "    \"avg_question_length\": np.mean(question_lengths),\n",
    "    \"num_samples\": len(results)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from the results and saving it to a CSV file. Also saving the file to W&B for tracking.\n",
    "\n",
    "out_df = pd.DataFrame(results)\n",
    "\n",
    "OUT_PATH = f\"/content/submission_part_{START}_{END}.csv\"\n",
    "out_df.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "wandb.save(OUT_PATH)\n",
    "print(\"Saved:\", OUT_PATH)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SShbFmPTWoP"
   },
   "source": [
    "## 5. Semantic Repair Using Cycle Consistency\n",
    "\n",
    "After initial generation, questions are evaluated using a cycle-consistency metric:\n",
    "- Question → regenerated answer\n",
    "- Cosine similarity between the regenerated answer and the original answer\n",
    "\n",
    "While most questions achieve high semantic alignment, a small fraction score poorly due to underspecification or incomplete generation.\n",
    "\n",
    "For these low-scoring cases:\n",
    "- A base instruction-tuned language model is used with a strong semantic prompt.\n",
    "- A new question is generated and evaluated.\n",
    "- The original question is replaced **only if** semantic similarity improves.\n",
    "\n",
    "This selective regeneration strategy ensures monotonic improvement while preserving high-quality outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentence-transformers wandb tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Weights & Biases for experiment tracking\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sota-ai-task3\",\n",
    "    name=\"base-model-semantic-repair\",\n",
    "    config={\n",
    "        \"model\": \"Llama-3.1-8B-Instruct (base)\",\n",
    "        \"strategy\": \"single-shot-semantic-prompt\",\n",
    "        \"replace_only_if_better\": True\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all submission CSV files into a single CSV\n",
    "\n",
    "import glob\n",
    "\n",
    "paths = sorted(glob.glob(\"/content/submission_part_*.csv\"))\n",
    "print(\"Found CSVs:\", len(paths))\n",
    "\n",
    "df = pd.concat([pd.read_csv(p) for p in paths], ignore_index=True)\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "df.head()\n",
    "\n",
    "FINAL_PATH = \"/content/combined_all.csv\"\n",
    "df[[\"quesid\", \"question\", \"cycle_score\"]].to_csv(FINAL_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the combined CSV and the test dataset to verify total rows\n",
    "\n",
    "df = pd.read_csv(FINAL_PATH)\n",
    "test_df = pd.read_csv(\"/content/test.csv\")\n",
    "\n",
    "print(\"Total rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying rows with cycle_score below the target threshold for potential repair\n",
    "\n",
    "TARGET_THRESHOLD = 0.6\n",
    "repair_df = df[df[\"cycle_score\"] < TARGET_THRESHOLD].copy()\n",
    "\n",
    "print(\"Rows to repair:\", len(repair_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the base Llama 3.1 8B Instruct model for regeneration of questions and answers in 4-bit quantized form for efficiency\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sentence transformer for embedding generation. Also defining helper functions to generate questions and compute cycle scores. The prompt for question generation focuses on reconstructing the most likely question from a given answer.\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def generate_question_base(answer):\n",
    "    prompt = f\"\"\"You are reconstructing an exam question from its answer.\n",
    "\n",
    "Your goal is to infer the MOST LIKELY question that would have caused the answer below.\n",
    "\n",
    "Requirements:\n",
    "- The question must target the central claim or concept of the answer.\n",
    "- The question must be specific enough that this answer directly responds to it.\n",
    "- Avoid generic questions.\n",
    "- Write exactly ONE question.\n",
    "- End with a question mark.\n",
    "- Do not include explanations, answers, or options.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Question:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=48,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    q = text.split(\"Question:\")[-1].strip()\n",
    "    q = q.split(\"?\")[0].strip() + \"?\"\n",
    "    return q\n",
    "\n",
    "\n",
    "def cycle_score(answer, question):\n",
    "    prompt = f\"\"\"Answer the following exam question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    regen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    regen = regen.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    e1 = embedder.encode(answer, convert_to_tensor=True)\n",
    "    e2 = embedder.encode(regen, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(e1, e2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main repair loop: regenerating questions and computing new cycle scores. If the new score is better, update the DataFrame and log the improvement.\n",
    "\n",
    "fixed = 0\n",
    "\n",
    "for idx, row in tqdm(repair_df.iterrows(), total=len(repair_df)):\n",
    "    qid = row[\"quesid\"]\n",
    "    old_q = row[\"question\"]\n",
    "    old_score = row[\"cycle_score\"]\n",
    "\n",
    "    answer = test_df.loc[test_df[\"quesid\"] == qid, \"ans\"].values[0]\n",
    "\n",
    "    new_q = generate_question_base(answer)\n",
    "    new_score = cycle_score(answer, new_q)\n",
    "\n",
    "    wandb.log({\n",
    "        \"old_score\": old_score,\n",
    "        \"new_score\": new_score,\n",
    "        \"question_length\": len(new_q.split())\n",
    "    })\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"OLD Q:\", old_q)\n",
    "    print(\"OLD SCORE:\", old_score)\n",
    "    print(\"NEW Q:\", new_q)\n",
    "    print(\"NEW SCORE:\", new_score)\n",
    "\n",
    "    if new_score > old_score: # only replace if better\n",
    "        df.loc[df[\"quesid\"] == qid, \"question\"] = new_q\n",
    "        df.loc[df[\"quesid\"] == qid, \"cycle_score\"] = new_score\n",
    "        fixed += 1\n",
    "        wandb.log({\"repair_success\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging final statistics after repair and saving the final combined CSV file\n",
    "\n",
    "wandb.log({\n",
    "    \"total_fixed\": fixed,\n",
    "    \"mean_cycle_score_after\": df[\"cycle_score\"].mean(),\n",
    "    \"median_cycle_score_after\": df[\"cycle_score\"].median()\n",
    "})\n",
    "\n",
    "print(\"Fixed:\", fixed)\n",
    "\n",
    "FINAL_PATH = \"/content/final_combined.csv\"\n",
    "df[[\"quesid\", \"question\", \"cycle_score\"]].to_csv(FINAL_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(FINAL_PATH)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSddR3T3dMxb"
   },
   "source": [
    "## 6. Final Analysis and Submission Preparation\n",
    "\n",
    "After completing generation and selective semantic repair, we obtain a final CSV containing the following columns:\n",
    "- `quesid`: unique identifier for each example\n",
    "- `question`: the reconstructed exam question\n",
    "- `cycle_score`: a semantic alignment score used internally for quality control\n",
    "\n",
    "### Qualitative Observations\n",
    "- The majority of generated questions are concise, specific, and directly target the central claim of the corresponding answer.\n",
    "- Questions with initially poor semantic alignment were selectively regenerated, leading to a strong overall distribution of semantic scores.\n",
    "- The final average cycle score is approximately **80%**, indicating high semantic consistency between answers and reconstructed questions.\n",
    "\n",
    "It is important to note that `cycle_score` is **not part of the official evaluation**. It is used purely as a diagnostic and quality-assurance signal during development to:\n",
    "- identify degenerate or underspecified questions,\n",
    "- guide selective regeneration,\n",
    "- ensure monotonic improvement without manual inspection.\n",
    "\n",
    "### Preparing the Final Submission\n",
    "For the Kaggle submission, only the required columns are retained:\n",
    "- `quesid`\n",
    "- `question`\n",
    "\n",
    "The `cycle_score` column is dropped prior to submission, as it is not expected by the evaluation system.\n",
    "\n",
    "The resulting CSV contains exactly one well-formed question per test example and constitutes the final submission artifact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading final combined CSV and preparing submission file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "final_df = pd.read_csv(\"/content/final_combined.csv\")\n",
    "\n",
    "print(\"Final dataset size:\", len(final_df))\n",
    "print(\"Average cycle score:\", final_df[\"cycle_score\"].mean())\n",
    "\n",
    "submission_df = final_df[[\"quesid\", \"question\"]]\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "submission_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the final submission by sorting questions in numeric order based on quesid\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load final CSV (with cycle_score still present)\n",
    "df = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "# Extract numeric index from quesid (e.g., test_123 -> 123)\n",
    "df[\"qid_num\"] = df[\"quesid\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Sort by numeric order\n",
    "df = df.sort_values(\"qid_num\").reset_index(drop=True)\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=[\"qid_num\"])\n",
    "\n",
    "# Sanity checks\n",
    "assert df[\"quesid\"].iloc[0] == \"test_0\"\n",
    "assert df[\"quesid\"].iloc[-1].startswith(\"test_\")\n",
    "\n",
    "# Prepare final submission (drop cycle_score)\n",
    "submission_df = df[[\"quesid\", \"question\"]]\n",
    "\n",
    "# Save\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "\n",
    "print(\"Final submission saved.\")\n",
    "print(\"Rows:\", len(submission_df))\n",
    "submission_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('final_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL_lcnVJv9Jf"
   },
   "source": [
    "## 7. Final Tail Repair for Low-Scoring Questions (≤ 0.67)\n",
    "\n",
    "After initial generation and selective semantic repair, the majority of reconstructed questions achieved strong semantic alignment with their corresponding answers. However, a small tail of examples still exhibited relatively low cycle-consistency scores (≤ 0.67).\n",
    "\n",
    "Rather than retraining models or applying broad changes, a targeted final repair phase was introduced to address only these low-confidence cases.\n",
    "\n",
    "### Motivation\n",
    "- Low cycle scores typically arise from underspecified or overly generic questions.\n",
    "- These cases often require stronger intent inference rather than additional training.\n",
    "- Since evaluation is semantic, improving the weakest tail can significantly increase robustness and overall performance.\n",
    "\n",
    "### Approach\n",
    "For each example with a cycle score ≤ 0.67:\n",
    "- A **strong inverse-QA prompt** is used with a base instruction-tuned language model.\n",
    "- The prompt explicitly emphasizes reconstructing the *original* exam question that directly caused the answer.\n",
    "- Exactly one new candidate question is generated.\n",
    "- The new question is evaluated using the same cycle-consistency metric.\n",
    "- The original question is replaced **only if** the new cycle score is strictly higher.\n",
    "\n",
    "### Key Properties\n",
    "- This process is deterministic and per-example independent.\n",
    "- High-quality questions are never overwritten.\n",
    "- Improvements are monotonic by construction.\n",
    "\n",
    "This final tail-focused refinement substantially improves semantic alignment in difficult cases, leading to a cleaner distribution of scores and a stronger overall submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentence-transformers wandb tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Weights & Biases for experiment tracking\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sota-ai-task3\",\n",
    "    name=\"final-strong-tail-repair\",\n",
    "    config={\n",
    "        \"threshold\": 0.67,\n",
    "        \"model\": \"Llama-3.1-8B-Instruct (base)\",\n",
    "        \"strategy\": \"single-shot-strong-inverse-prompt\",\n",
    "        \"replace_only_if_better\": True\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with quesid, question, cycle_score\n",
    "df = pd.read_csv(\"/content/final_combined.csv\")\n",
    "\n",
    "# Test answers (quesid, ans)\n",
    "test_df = pd.read_csv(\"/content/test.csv\")\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Mean cycle score (before):\", df[\"cycle_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying rows with cycle_score below the stronger threshold for potential repair\n",
    "\n",
    "THRESHOLD = 0.67\n",
    "repair_df = df[df[\"cycle_score\"] <= THRESHOLD].copy()\n",
    "\n",
    "print(\"Rows to repair:\", len(repair_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the base Llama 3.1 8B Instruct model in 4-bit quantized format for efficient inference. Also loading the tokenizer.\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sentence transformer for embedding generation.\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generate_strong_question function to reconstruct the original exam question from a given answer using a strong inverse prompt. The focus is on inferring the exact question that elicited the provided answer by providing a strong prompt with clear instructions.\n",
    "\n",
    "def generate_strong_question(answer):\n",
    "    prompt = f\"\"\"You are reconstructing the ORIGINAL exam question that directly caused the answer below.\n",
    "\n",
    "Important:\n",
    "- This answer was written in response to ONE specific academic question.\n",
    "- Your job is to infer that exact intent.\n",
    "- The question must be specific enough that this answer is a direct and complete response.\n",
    "- Generic or vague questions are incorrect.\n",
    "\n",
    "Rules:\n",
    "- Write exactly ONE question.\n",
    "- Do NOT explain.\n",
    "- Do NOT add options.\n",
    "- Do NOT repeat phrases unnecessarily.\n",
    "- End with a single question mark.\n",
    "\n",
    "Think carefully about:\n",
    "- the main claim being defended or explained\n",
    "- any named philosopher, theory, or concept\n",
    "- whether the answer is explaining a definition, a distinction, a criticism, or an implication\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Original Question:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    q = text.split(\"Original Question:\")[-1].strip()\n",
    "\n",
    "    # Hard sanitation\n",
    "    q = q.split(\"?\")[0].strip() + \"?\"\n",
    "    q = re.sub(r\"\\s+\", \" \", q)\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the compute_cycle_score function to evaluate the semantic similarity between the original answer and the regenerated answer based on the newly generated question.\n",
    "\n",
    "def compute_cycle_score(original_answer, question):\n",
    "    prompt = f\"\"\"Answer the following academic exam question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    regen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    regen = regen.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    e1 = embedder.encode(original_answer, convert_to_tensor=True)\n",
    "    e2 = embedder.encode(regen, convert_to_tensor=True)\n",
    "\n",
    "    return float(util.cos_sim(e1, e2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main repair loop: regenerating questions and computing new cycle scores. If the new score is better, update the DataFrame and log the improvement.\n",
    "\n",
    "fixed = 0\n",
    "skipped_short = 0\n",
    "\n",
    "for idx, row in tqdm(repair_df.iterrows(), total=len(repair_df)):\n",
    "    qid = row[\"quesid\"]\n",
    "    old_q = row[\"question\"]\n",
    "    old_score = row[\"cycle_score\"]\n",
    "\n",
    "    answer = test_df.loc[test_df[\"quesid\"] == qid, \"ans\"].values[0]\n",
    "\n",
    "    new_q = generate_strong_question(answer)\n",
    "\n",
    "    # Sanity check: avoid garbage\n",
    "    if len(new_q.split()) < 8:\n",
    "        skipped_short += 1\n",
    "        wandb.log({\"skipped_short_question\": 1})\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        new_score = compute_cycle_score(answer, new_q)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # Logging\n",
    "    wandb.log({\n",
    "        \"old_score\": old_score,\n",
    "        \"new_score\": new_score,\n",
    "        \"delta\": new_score - old_score,\n",
    "        \"question_len\": len(new_q.split())\n",
    "    })\n",
    "\n",
    "    print(\"=\" * 90)\n",
    "    print(\"QUESID:\", qid)\n",
    "    print(\"OLD Q:\", old_q)\n",
    "    print(\"OLD SCORE:\", round(old_score, 4))\n",
    "    print(\"NEW Q:\", new_q)\n",
    "    print(\"NEW SCORE:\", round(new_score, 4))\n",
    "\n",
    "    # Replace only if strictly better\n",
    "    if new_score > old_score:\n",
    "        df.loc[df[\"quesid\"] == qid, \"question\"] = new_q\n",
    "        df.loc[df[\"quesid\"] == qid, \"cycle_score\"] = new_score\n",
    "        fixed += 1\n",
    "        wandb.log({\"repair_success\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final logging after repair and saving the updated CSV file\n",
    "\n",
    "print(\"Repaired rows:\", fixed)\n",
    "print(\"Skipped (too short):\", skipped_short)\n",
    "\n",
    "wandb.log({\n",
    "    \"total_repaired\": fixed,\n",
    "    \"mean_cycle_score_after\": df[\"cycle_score\"].mean(),\n",
    "    \"median_cycle_score_after\": df[\"cycle_score\"].median()\n",
    "})\n",
    "\n",
    "# Save updated CSV (keep cycle_score for audit)\n",
    "df.to_csv(\"final_after_strong_repair.csv\", index=False)\n",
    "wandb.save(\"final_after_strong_repair.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks on the final DataFrame and finishing the W&B run\n",
    "\n",
    "assert df[\"question\"].str.endswith(\"?\").all()\n",
    "assert df[\"quesid\"].nunique() == len(df)\n",
    "\n",
    "df.sample(5)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('final_after_strong_repair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the final submission by sorting questions in numeric order based on quesid after strong repair\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load final CSV (with cycle_score still present)\n",
    "df = pd.read_csv(\"/content/final_after_strong_repair.csv\")\n",
    "\n",
    "# Extract numeric index from quesid (e.g., test_123 -> 123)\n",
    "df[\"qid_num\"] = df[\"quesid\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Sort by numeric order\n",
    "df = df.sort_values(\"qid_num\").reset_index(drop=True)\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=[\"qid_num\"])\n",
    "\n",
    "# Sanity checks\n",
    "assert df[\"quesid\"].iloc[0] == \"test_0\"\n",
    "assert df[\"quesid\"].iloc[-1].startswith(\"test_\")\n",
    "\n",
    "# Prepare final submission (drop cycle_score)\n",
    "submission_df = df[[\"quesid\", \"question\"]]\n",
    "\n",
    "# Save\n",
    "submission_df.to_csv(\"strong_repairs_submission.csv\", index=False)\n",
    "\n",
    "print(\"Final submission saved.\")\n",
    "print(\"Rows:\", len(submission_df))\n",
    "submission_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook presents a complete, principled solution to the inverse question–answering task in the SOTA-AI December Challenge. Starting from an unlabeled dataset of model-generated answers, the approach progressively builds structure through synthetic supervision, parameter-efficient fine-tuning, deterministic inference, and targeted semantic repair.\n",
    "\n",
    "Key takeaways from this work include:\n",
    "- Inverse QA is fundamentally a semantic reasoning problem rather than a surface-form generation task.\n",
    "- Small, high-quality synthetic datasets can be more effective than large noisy ones when combined with careful filtering.\n",
    "- Cycle consistency serves as a practical proxy for semantic alignment and enables safe, monotonic post-processing.\n",
    "- Selective tail-focused refinement can substantially improve robustness without risking regression.\n",
    "\n",
    "All stages of the pipeline are designed to be modular, interpretable, and reproducible. While practical execution may involve batch-wise processing under constrained resources, the methodology itself is conceptually complete and generalizes to the full dataset.\n",
    "\n",
    "Thank you for this challenge SOTA-AI Community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
